{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=[]    \n",
    "fileObject = open('untitled.txt', 'r')\n",
    "for lines in fileObject.readlines():\n",
    "    lines=txt.findall(lines)\n",
    "    if lines:\n",
    "        lines=lines[0].split(')')[-1]\n",
    "        text.append('['+lines+']')\n",
    "fileObject.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileObject = open('clean_poem.txt', 'w')  \n",
    "for t in text:  \n",
    "    fileObject.write(t)  \n",
    "    fileObject.write('\\n')  \n",
    "fileObject.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-1d515cb017d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfileObject\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlines\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfileObject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msentences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'path' is not defined"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "ma=0\n",
    "fileObject = open(path, 'r')\n",
    "for lines in fileObject.readlines():\n",
    "    sentences.append(lines.decode('utf-8'))\n",
    "    ma=max(ma,len(lines.decode('utf-8')))\n",
    "fileObject.close()\n",
    "ma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import tensorflow as tf\n",
    "# gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.5)\n",
    "# sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "# from keras import backend as K\n",
    "# import tensorflow as tf\n",
    "# config = tf.ConfigProto()\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "# session = tf.Session(config=config)\n",
    "# K.set_session(session)\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "import numpy as np\n",
    "import random, sys\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow.contrib import rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opening txt\n",
      "corpus length: 13347\n",
      "nb sequences: 4436\n",
      "('dictionary_size: ', 2511)\n"
     ]
    }
   ],
   "source": [
    "path = './poetryFromTang.txt'\n",
    "print 'opening txt'\n",
    "text = open(path).read().lower().decode('utf-8').replace('\\n','').replace('[','').replace(']','').replace(u'，','').replace(u'。','')\n",
    "print 'corpus length:', len(text)\n",
    "\n",
    "chars = Counter(text).most_common()\n",
    "dictionary_size=len(chars)\n",
    "embedding_size=64\n",
    "timesteps=40\n",
    "char_indices = dict((c[0], i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c[0]) for i, c in enumerate(chars))\n",
    "\n",
    "step = 3\n",
    "num_hidden=embedding_size\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - timesteps, step):\n",
    "    sentences.append([map(char_indices.get,text[i : i + timesteps])])\n",
    "    next_chars.append([map(char_indices.get,text[i+1:i+1 + timesteps])])\n",
    "print 'nb sequences:', len(sentences)\n",
    "\n",
    "print('dictionary_size: ',dictionary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "巴山上峡重复重阳台碧峭十二峰荆王猎时逢暮雨夜卧高丘梦神女轻红流烟湿艳姿行云飞去明星稀目极魂断望不见猿啼三声泪沾衣见尽数万里不闻三声猿但飞萧萧雨中有亭亭魂千载楚襄恨遗文宋玉言至今青冥里云结深闺门碧丛丛高插天大江翻澜神曳烟楚魂寻梦风飔然晓风飞雨生苔钱瑶姬一去一千年丁香筇竹啼老猿古祠近月蟾桂寒椒花坠红湿云间巫山高巫女妖雨为暮兮云为朝楚王憔悴魂欲销秋猿嗥嗥日将夕红霞紫烟凝老壁千岩万壑花皆坼但恐芳菲无正色不知今古行人行几人经此无秋情云深庙远不可觅十二峰头插天碧君不见黄河之水天上来奔流到海不复回君不见高堂明镜悲白发朝如青丝暮成雪人生得意须尽欢莫使金尊空对月天生我材必有用千金散尽还复来烹羊宰牛且为乐会须一饮三百杯岑夫子丹丘生将进酒杯莫停与君歌一曲请君为我侧耳听钟鼓馔玉不足贵但愿长醉不复醒古来圣贤皆寂寞惟有饮者留其名陈王昔时宴平乐斗酒十千恣欢谑主人何为言少钱径须酤取对君酌五花马千金裘呼儿将出换美酒与尔同销万古愁将进酒将进酒酒中有毒鸩主父言之主父伤主母母为妾地父妾天仰天俯地不忍言佯为僵踣主父前主父不知加妾鞭旁人知妾为主说主将泪洗鞭头血推摧主母牵下堂扶妾遣升堂上床将进酒酒中无毒令主寿愿主回思归主母遣妾如此事主父妾为此事人偶知自惭不密方自悲主今颠倒安置妾贪天僭地谁不为琉璃钟琥珀浓小槽酒滴真珠红烹龙炮凤玉脂泣罗屏绣幕围香风吹龙笛击鼍鼓皓齿歌细腰舞况是青春日将暮桃花乱落如红雨劝君终日酩酊醉酒不到刘伶坟上土君马黄我马白马色虽不同人心本无隔共作游冶盘双行洛阳陌长剑既照曜高冠何赩赫各有千金裘俱为五侯客猛虎落陷阱壮夫时屈厄相知在急难独好亦何益何地早芳菲宛在长门殿夭桃色若绶秾李光如练啼鸟弄花疏游蜂饮香遍叹息春风起飘零君不见芳树本多奇年华复在斯结翠成新幄开红满旧枝风归花历乱日度影参差容色朝朝落思君君不知玉花珍簟上金缕画屏开晓月怜筝柱春风忆镜台筝柱春风吹晓月芳树落花朝暝歇稿砧刀头未有时攀条拭泪坐相思迢迢芳园树列映清池曲对此伤人心还如故时绿风条洒馀霭露叶承新旭佳人不再攀下有往来躅 芳树已寥落孤英尤可嘉可怜团团叶盖覆深深花游蜂竞攒刺斗雀亦纷拏天生细碎物不爱好光华非无歼殄法念尔有生涯春雷一声发惊燕亦惊蛇清池养神蔡已复长虾蟆雨露贵平施吾其春草芽细蕊慢逐风暖香闲破鼻青帝固有心时时动人意去年高枝犹压地今年低枝已憔悴吾所以见造化之权变通之理春夏作头秋冬为尾循环反复无穷已今生长短同一轨若使威可以制力可以止秦皇不肯敛手下沙丘孟贲不合低头入蒿里伊人强猛犹如此顾我劳生何足恃但愿开素袍倾绿蚁陶陶兀兀大醉于青冥白昼间任他上是天下是地 君子事行役再空芳岁期美人旷延伫万里浮云思园槿绽红艳郊桑柔绿滋坐看长夏晚秋月生罗帏我思仙人乃在碧海之东隅海寒多天风白波连山倒蓬壶长鲸喷涌不可涉抚心茫茫泪如珠西来青鸟东飞去愿寄一书谢麻姑桔槔烽火昼不灭客路迢迢信难越古镇刀攒万片霜寒江浪起千堆雪此时西去定如何空使南心远凄切 当时我醉美人家美人颜色娇如花今日美人弃我去青楼珠箔天之涯天涯娟娟常娥月三五二八盈又缺翠眉蝉鬓生别离一望不见心断绝心断绝几千里梦中醉卧巫山云觉来泪滴湘江水湘江两岸花木深美人不见愁人心含愁更奏绿绮琴调高弦绝无知音美人兮美人不知为暮雨兮为朝云相思一夜梅花发忽到窗前疑是君借问江上柳青青为谁春空游昨日地不见昨日人缭绕万家井往来车马尘莫道无相识要非心所亲朝亦有所思暮亦有所思登楼望君处蔼蔼浮云飞浮云遮却阳关道向晚谁知妾怀抱玉井苍苔春院深桐花落地无人扫辟邪伎作鼓吹惊雉子班之奏曲成喔咿振迅欲飞鸣扇锦翼雄风生双雌同饮啄趫悍谁能争乍向草中耿介死不求黄金笼下生所贵旷士怀朗然合太清 高台暂俯临飞翼耸轻音浮光随日度漾影逐波深迥瞰周平野开怀畅远襟独此三休上还伤千岁心 临高台高台迢递绝浮埃瑶轩绮构何崔嵬鸾歌凤吹清且哀俯瞰长安道萋萋御沟草斜对甘泉路苍苍茂陵树高台四望同帝乡佳气郁葱葱紫阁丹楼纷照曜璧房锦殿相玲珑东弥长乐观西指未央宫赤城映朝日绿树摇春风旗亭百队开新市甲第千甍分戚里朱轮翠盖不胜春叠树层楹相对起复有青楼大道中绣户文窗雕绮栊锦衣昼不襞罗帏夕未空歌屏朝掩翠妆镜晚窥红为吾安宝髻蛾眉罢花丛狭路尘间黯将暮云间月色明如素鸳鸯池上两两飞凤凰楼下双双度物色正如此佳期那不顾银鞍绣毂盛繁华可怜今夜宿倡家倡家少妇不须嚬东园桃李片时春君看旧日高台处柏梁铜雀生黄尘凉风吹远念使我升高台宁知数片云不是旧山来故人天一涯久客殊未回雁来不得书空寄声哀哀穿屋穿墙不知止争树争巢入营死林间公子挟弹弓一丸致毙花丛里小雏黄口未有知青天不解高高飞虞人设网当要路白日啾嘲祸万机朝日敛红烟垂竿向绿川人疑天上坐鱼似镜中悬避楫时惊透猜钩每误牵湍危不理辖潭静欲留船钓玉君徒尚征金我未贤为看芳饵下贪得会无全汉将承恩西破戎捷书先奏未央宫天子预开麟阁待只今谁数贰师功官军西出过楼兰营幕傍临月窟寒蒲海晓霜凝剑尾葱山夜雪扑旌竿鸣笳擂鼓拥回军破国平蕃昔未闻大夫鹊印摇边月天将龙旗掣海云月落辕门鼓角鸣千群面缚出蕃城洗兵鱼海云迎阵秣马龙堆月照营蕃军遥见汉家营满谷连山遍哭声万箭千刀一夜杀平明流血浸空城暮雨旌旗湿未干胡尘白草日光寒昨夜将军连晓战蕃军只见马空鞍晋阳武奋义威炀之渝德焉归氓毕屠绥者谁皇烈烈专天机号以仁扬其旗日之升九土晞斥田圻流洪辉有其二翼馀隋斫枭骜连熊螭枯以肉勍者羸后土荡玄穹弥合之育莽然施惟德辅庆无期兽之穷奔大麓天厚黄德狙犷服甲之櫜弓弭矢箙皇旅靖敌逾蹙自亡其徒匪予戮屈rh猛虔栗栗縻以尺组啖以秩黎之阳土茫茫富兵戎盈仓箱乏者德莫能享驱豺兕授我疆 战武牢动河朔逆之助图掎角怒鷇麛抗乔岳翘萌牙傲霜雹王谋内定申掌握铺施芟夷二主缚惮华戎廓封略命之瞢卑以斫归有德唯先觉泾水黄陇野茫负太白腾天狼有鸟鸷立羽翼张钩喙决前钜趯傍；怒飞饥啸翾不可当老雄死子复良巢岐饮渭肆翱翔顿地纮提天纲列缺掉帜招摇耀铓鬼神来助梦嘉祥脑涂原野魂飞扬星辰复恢一方奔鲸沛荡海垠吐霓翳日腥浮云帝怒下顾哀垫昏授以神柄推元臣手援天矛截修鳞披攘蒙霿开海门地平水静浮天垠羲和显耀乘清氛赫炎溥畅融大钧苞枿ba矣惟根之蟠弥巴蔽荆负南极以安曰我旧梁氏辑绥艰难江汉之阻都邑固以完圣人作神武用有臣勇智奋不以众投迹死地谋猷纵化敌为家虑则中浩浩海裔不威而同系缧降王定厥功澶漫万里宣唐风蛮夷九译咸来从凯旋金奏象形容震赫万国罔不龚河右澶漫顽为之魁王师如雷震昆仑以颓上聋下聪骜不可回助仇抗有德惟人之灾乃溃乃奋执缚归厥命万室蒙其仁一夫则病濡以鸿泽皇之圣威畏德怀功以定顺之于理物咸遂厥性铁山碎大漠舒二虏劲连穹庐背北海专坤隅岁来侵边或傅于都天子命元帅奋其雄图破定襄降魁渠穷竟窟宅斥余吾百蛮破胆边氓苏威武辉耀明鬼区利泽弥万祀功不可逾官臣拜手惟帝之谟本邦伊晋惟时不靖根柢之摇枝叶攸病守臣不任勩于神圣惟钺之兴翦焉则定洪惟我理式和以敬群顽既夷庶绩咸正皇谟载大惟人之庆吐谷浑盛强背西海以夸岁侵扰我疆退匿险且遐帝谓神武师往征靖皇家烈烈旆其旗熊虎杂龙蛇王旅千万人衔枚默无哗束刃逾山徼张翼纵漠沙一举刈膻腥尸骸积如麻除恶务本根况敢遗萌芽洋洋西海水威命穷天涯系虏来王都犒乐穷休嘉登高望还师竟野如春华行者靡不归亲戚讙要遮凯旋献清庙万国思无邪麹氏雄西北别绝臣外区既恃远且险纵傲不我虞烈烈王者师熊螭以为徒龙旂翻海浪馹骑驰坤隅贲育搏婴儿一扫不复馀平沙际天极但见黄云驱臣靖执长缨智勇伏囚拘文皇南面坐夷狄千群趋咸称天子神往古不得俱献号天可汗以覆我国都兵戎不交害各保性与躯东蛮有谢氏冠带理海中已言我异世虽圣莫能通王卒如飞翰鹏鶱骇群龙轰然自天坠乃信神武功系虏君臣人累累来自东无思不服从唐业如山崇百辟拜稽首咸愿图形容如周王会书永永传无穷睢盱万状乖咿嗢九译重广轮抚四海浩浩知皇风歌诗铙鼓间以壮我元戎 片玉来夸楚治中作主人江山增润色词赋动阳春别馆当虚敞离情任吐伸因声两京旧谁念卧漳滨楚关望秦国相去千里馀州县勤王事山河转使车祖筵江上列离恨别前书愿及芳年赏娇莺二月初早闻牛渚咏今见鶺鴒心羽翼嗟零落悲鸣别故林苍梧白云远烟水洞庭深万里独飞去南风迟尔音旧国余归楚新年子北征挂帆愁海路分手恋朋情日夕故园意汀洲春草生何时一杯酒重与季鹰倾 吾道昧所适驱车还向东主人开旧馆留客醉新丰树绕温泉绿尘遮晚日红拂衣从此去高步蹑华嵩何幸遇休明观光来上京相逢武陵客独送豫章行随牒牵黄绶离群会墨卿江南佳丽地山水旧难名南国辛居士言归旧竹林未逢调鼎用徒有济川心予亦忘机者田园在汉阴因君故乡去遥寄式微吟惜尔怀其宝迷邦倦客游江山历全楚河洛越成周道路疲千里乡园老一丘知君命不偶同病亦同忧 奉使推能者勤王不暂闲观风随按察乘骑度荆关送别登何处开筵旧岘山征轩明日远空望郢门间导漾自嶓冢东流为汉川维桑君有意解缆我开筵云雨从兹别林端意渺然尺书能不吝时望鲤鱼传西上游江西临流恨解携千山叠成嶂万水泻为溪石浅流难溯藤长险易跻谁怜问津者岁晏此中迷士有不得志栖栖吴楚间广陵相遇罢彭蠡泛舟还樯出江中树波连海上山风帆明日远何处更追攀 献策金门去承欢彩服违以吾一日长念尔聚星稀昏定须温席寒多未授衣桂枝如已擢早逐雁南飞白日既云暮朱颜亦已酡画堂初点烛金幌半垂罗长袖平阳曲新声子夜歌从来惯留客兹夕为谁多侧听弦歌宰文书游夏徒故园欣赏竹为邑幸来苏华省曾联事仙舟复与俱欲知临泛久荷露渐成珠邑有弦歌宰翔鸾狎野鸥眷言华省旧暂拂海池游郁岛藏深竹前溪对舞楼更闻书即事云物是清秋 甲第开金穴荣期乐自多枥嘶支遁马池养右军鹅竹引携琴入花邀载酒过山公来取醉时唱接z5歌言避一时暑池亭五月开喜逢金马客同饮玉人杯舞鹤乘轩至游鱼拥钓来座中殊未起箫管莫相催林卧愁春尽开轩览物华忽逢青鸟使邀入赤松家丹灶初开火仙桃正落花童颜若可驻何惜醉流霞瑞雪初盈尺寒宵始半更列筵邀酒伴刻烛限诗成香炭金炉暖娇弦玉指清醉来方欲卧不觉晓鸡鸣 世业传珪组江城佐股肱高斋征学问虚薄滥先登讲论陪诸子文章得旧朋士元多赏激衰病恨无能人事有代谢往来成古今江山留胜迹我辈复登临水落鱼梁浅天寒梦泽深羊公碑字在读罢泪沾襟水楼一登眺半出青林高帟幕英僚敞芳筵下客叨山藏伯禹穴城压伍胥涛今日观溟涨垂纶学钓鳌吾友太乙子餐霞卧赤城欲寻华顶去不惮恶溪名歇马凭云宿扬帆截海行高高翠微里遥见石梁横 秋入诗人意巴歌和者稀泛湖同逸旅吟会是思归白简徒推荐沧洲已拂衣杳冥云外去谁不羡鸿飞挂席几千里名山都未逢泊舟浔阳郭始见香炉峰尝读远公传永怀尘外踪东林精舍近日暮但闻钟独步人何在嵩阳有故楼岁寒问耆旧行县拥诸侯林莽北弥望沮漳东会流客中遇知己无复越乡忧武陵川路狭前棹入花林莫测幽源里仙家信几深水回青嶂合云度绿溪阴坐听闲猿啸弥清尘外心 百里闻雷震鸣弦暂辍弹府中连骑出江上待潮观照日秋云迥浮天渤澥宽惊涛来似雪一坐凛生寒主人新邸第相国旧池台馆是招贤辟楼因教舞开轩车人已散箫管凤初来今日龙门下谁知文举才水亭凉气多闲棹晚来过涧影见松竹潭香闻芰荷野童扶醉舞山鸟助酣歌幽赏未云遍烟光奈夕何故人来自远邑宰复初临执手恨为别同舟无异心沿洄洲渚趣演漾弦歌音谁识躬耕者年年梁甫吟 共喜年华好来游水石间烟容开远树春色满幽山壶酒朋情洽琴歌野兴闲莫愁归路暝招月伴人还南纪西江阔皇华御史雄截流宁假楫挂席自生风僚寀争攀鹢鱼龙亦避骢坐听白雪唱翻入棹歌中万山青嶂曲千骑使君游神女鸣环佩仙郎接献酬遍观云梦野自爱江城楼何必东南守空传沈隐侯海亭秋日望委曲见江山染翰聊题壁倾壶一解颜歌逢彭泽令归赏故园间予亦将琴史栖迟共取闲 河县柳林边河桥晚泊船文叨才子会官喜故人连笑语同今夕轻肥异往年晨风理归棹吴楚各依然傲吏非凡吏名流即道流隐居不可见高论莫能酬水接仙源近山藏鬼谷幽再来迷处所花下问渔舟龙象经行处山腰度石关屡迷青嶂合时爱绿萝闲宴息花林下高谈竹屿间寥寥隔尘事疑是入鸡山欣逢柏台友共谒聪公禅石室无人到绳床见虎眠阴崖常抱雪枯涧为生泉出处虽云异同欢在法筵 出谷未停午到家日已曛回瞻下山路但见牛羊群樵子暗相失草虫寒不闻衡门犹未掩伫立望夫君夏日茅斋里无风坐亦凉竹林深笋穊藤架引梢长燕觅巢窠处蜂来造蜜房物华皆可玩花蕊四时芳释子弥天秀将军武库才横行塞北尽独步汉南来贝叶传金口山楼作赋开因君振嘉藻江楚气雄哉误入桃源里初怜竹径深方知仙子宅未有世人寻舞鹤过闲砌飞猿啸密林渐通玄妙理深得坐忘心 支遁初求道深公笑买山何如石岩趣自入户庭间苔涧春泉满萝轩夜月闲能令许玄度吟卧不知还人事一朝尽荒芜三径休始闻漳浦卧奄作岱宗游池水犹含墨风云已落秋今宵泉壑里何处觅藏舟彭泽先生柳山阴道士鹅我来从所好停策汉阴多重以观鱼乐因之鼓枻歌崔徐迹未朽千载揖清波带雪梅初暖含烟柳尚青来窥童子偈得听法王经会理知无我观空厌有形迷心应觉悟客思未遑宁 给园支遁隐虚寂养身和春晚群木秀间关黄鸟歌林栖居士竹池养右军鹅炎月北窗下清风期再过义公习禅处结构依空林户外一峰秀阶前群壑深夕阳连雨足空翠落庭阴看取莲花净应知不染心白鹤青岩半幽人有隐居阶庭空水石林壑罢樵渔岁月青松老风霜苦竹疏睹兹怀旧业回策返吾庐精舍买金开流泉绕砌回芰荷薰讲席松柏映香台法雨晴飞去天花昼下来谈玄殊未已归骑夕阳催 池上青莲宇林间白马泉故人成异物过客独潸然既礼新松塔还寻旧石筵平生竹如意犹挂草堂前与君园庐并微尚颇亦同耕钓方自逸壶觞趣不空门无俗士驾人有上皇风何处先贤传惟称庞德公弱岁早登龙今来喜再逢如何春月柳犹忆岁寒松烟火临寒食笙歌达曙钟喧喧斗鸡道行乐羡朋从闻就庞公隐移居近洞湖兴来林是竹归卧谷名愚挂席樵风便开轩琴月孤岁寒何用赏霜落故园芜 府僚能枉驾家酝复新开落日池上酌清风松下来厨人具鸡黍稚子摘杨梅谁道山公醉犹能骑马回折戟沈沙铁未销自将磨洗认前朝东风不与周郎便铜雀春深锁二乔日旗龙旆想飘扬一索功高缚楚王直是超然五湖客未如终始郭汾阳贱子来千里明公去一麾可能休涕泪岂独感恩知草木穷秋后山川落照时如何望故国驱马却迟迟一笑五云溪上舟跳丸日月十经秋鬓衰酒减欲谁泥迹辱魂惭好自尤梦寐几回迷蛱蝶文章应解伴牢愁无穷尘土无聊事不得清言解不休烟笼寒水月笼沙夜泊秦淮近酒家商女不知亡国恨隔江犹唱后庭花萧萧山路穷秋雨淅淅溪风一岸蒲为问寒沙新到雁来时还下杜陵无细腰宫里露桃新脉脉无言度几春至竟息亡缘底事可怜金谷坠楼人雪涨前溪水啼声已绕滩梅衰未减态春嫩不禁寒迹去梦一觉年来事百般闻君亦多感何处倚阑干平生自许少尘埃为吏尘中势自回朱绂久惭官借与白题还叹老将来须知世路难轻进岂是君门不大开霄汉几多同学伴可怜头角尽卿材缄书报子玉为我谢平津自愧扫门士谁为乞火人词臣陪羽猎战将骋骐驎两地差池恨江汀醉送君芳草渡头微雨时万株杨柳拂波垂蒲根水暖雁初浴梅径香寒蜂未知辞客倚风吟暗淡使君回马湿旌旗江南仲蔚多情调怅望春阴几首诗江湖醉渡十年春牛渚山边六问津历阳前事知何实高位纷纷见陷人胜败兵家事不期包羞忍耻是男儿孙家兄弟晋龙骧驰骋功名业帝王至竟江山谁是主苔矶空属钓鱼郎发匀肉好生春岭截玉钻星寄使君檀的染时痕半月落梅飘处响穿云楼中威凤倾冠听沙上惊鸿掠水分遥想紫泥封诏罢夜深应隔禁墙闻青山隐隐水迢迢秋尽江南草木凋二十四桥明月夜玉人何处教吹箫故人别来面如雪一榻拂云秋影中玉白花红三百首五陵谁唱与春风贾傅松醪酒秋来美更香怜君片云思一去绕潇湘一渠东注芳华苑苑锁池塘百岁空水殿半倾蟾口涩为谁流下蓼花中锦缆龙舟隋炀帝平台复道汉梁王游人闲起前朝念折柳孤吟断杀肠千里长河初冻时玉珂瑶珮响参差浮生却似冰底水日夜东流人不知七子论诗谁似公曹刘须在指挥中荐衡昔日知文举乞火无人作蒯通北极楼台长挂梦西江波浪远吞空可怜故国三千里虚唱歌词满六宫大夫官重醉江东潇洒名儒振古风文石陛前辞圣主碧云天外作冥鸿五言宁谢颜光禄百岁须齐卫武公再拜宜同丈人行过庭交分有无同水接西江天外声小斋松影拂云平何人教我吹长笛与倚春风弄月明广文遗韵留樗散鸡犬图书共一船自说江湖不归事阻风中酒过年年三吴裂婺女九锡狱孤儿霸主业未半本朝心是谁永安宫受诏筹笔驿沉思画地乾坤在濡毫胜负知艰难同草创得失计毫厘寂默经千虑分明浑一期川流萦智思山耸助扶持慷慨匡时略从容问罪师褒中秋鼓角渭曲晚旌旗仗义悬无敌鸣攻故有辞若非天夺去岂复虑能支子夜星才落鸿毛鼎便移邮亭世自换白日事长垂何处躬耕者犹题殄瘁诗 邮亭寄人世人世寄邮亭何如自筹度鸿路有冥冥少微星动照春云魏阙衡门路自分倏去忽来应有意世间尘土谩疑君调高银字声还侧物比柯亭韵校奇寄与玉人天上去桓将军见不教吹历阳崔太守何日不含情恩义同钟李埙篪实弟兄光尘能混合擘画最分明台阁仁贤誉闺门孝友声西方像教毁南海绣衣行金橐宁回顾珠簟肯一枨只宜裁密诏何自取专城进退无非道徊翔必有名好风初婉软离思苦萦盈金马旧游贵桐庐春水生雨侵寒牖梦梅引冻醪倾共祝中兴主高歌唱太平纨袴不饿死儒冠多误身丈人试静听贱子请具陈甫昔少年日早充观国宾读书破万卷下笔如有神赋料扬雄敌诗看子建亲李邕求识面王翰愿卜邻自谓颇挺出立登要路津致君尧舜上再使风俗淳此意竟萧条行歌非隐沦骑驴三十载旅食京华春朝扣富儿门暮随肥马尘残杯与冷炙到处潜悲辛主上顷见征欻然欲求伸青冥却垂翅蹭蹬无纵鳞甚愧丈人厚甚知丈人真每于百僚上猥诵佳句新窃效贡公喜难甘原宪贫焉能心怏怏只是走踆踆今欲东入海即将西去秦尚怜终南山回首清渭滨常拟报一饭况怀辞大臣白鸥没浩荡万里谁能驯崆峒小麦熟且愿休王师请公问主将焉用穷荒为饥鹰未饱肉侧翅随人飞高生跨鞍马有似幽并儿脱身簿尉中始与捶楚辞借问今何官触热向武威答云一书记所愧国士知人实不易知更须慎其仪十年出幕府自可持旌麾此行既特达足以慰所思男儿功名遂亦在老大时常恨结欢浅各在天一涯又如参与商惨惨中肠悲惊风吹鸿鹄不得相追随黄尘翳沙漠念子何当归边城有馀力早寄从军诗二年客东都所历厌机巧野人对膻腥蔬食常不饱岂无青精饭使我颜色好苦乏大药资山林迹如扫李侯金闺彦脱身事幽讨亦有梁宋游方期拾瑶草已从招提游更宿招提境阴壑生虚籁月林散清影天阙象纬逼云卧衣裳冷欲觉闻晨钟令人发深省岱宗夫如何齐鲁青未了造化钟神秀阴阳割昏晓荡胸生曾云决眦入归鸟会当凌绝顶一览众山小东藩驻皂盖北渚凌青荷海内此亭古济南名士多云山已发兴玉佩仍当歌修竹不受暑交流空涌波蕴真惬所遇落日将如何贵贱俱物役从公难重过新亭结构罢隐见清湖阴迹籍台观旧气溟海岳深圆荷想自昔遗堞感至今芳宴此时具哀丝千古心主称寿尊客筵秩宴北林不阻蓬荜兴得兼梁甫吟故人昔隐东蒙峰已佩含景苍精龙故人今居子午谷独在阴崖结茅屋屋前太古玄都坛青石漠漠常风寒子规夜啼山竹裂王母昼下云旗翻知君此计成长往芝草琅玕日应长铁锁高垂不可攀致身福地何萧爽今夕何夕岁云徂更长烛明不可孤咸阳客舍一事无相与博塞为欢娱冯陵大叫呼五白袒跣不肯成枭卢英雄有时亦如此邂逅岂即非良图君莫笑刘毅从来布衣愿家无儋石输百万翻手作云覆手雨纷纷轻薄何须数君不见管鲍贫时交此道今人弃如土车辚辚马萧萧行人弓箭各在腰耶娘妻子走相送尘埃不见咸阳桥牵衣顿足阑道哭哭声直上干云霄道傍过者问行人行人但云点行频或从十五北防河便至四十西营田去时里正与裹头归来头白还戍边边亭流血成海水武皇开边意未已君不闻汉家山东二百州千村万落生荆杞纵有健妇把锄犁禾生陇亩无东西况复秦兵耐苦战被驱不异犬与鸡长者虽有问役夫敢申恨且如今年冬未休关西卒县官急索租租税从何出信知生男恶反是生女好生女犹是嫁比邻生男埋没随百草君不见青海头古来白骨无人收新鬼烦冤旧鬼哭天阴雨湿声啾啾安西都护胡青骢声价欻然来向东此马临阵久无敌与人一心成大功功成惠养随所致飘飘远自流沙至雄姿未受伏枥恩猛气犹思战场利腕促蹄高如踣铁交河几蹴曾冰裂五花散作云满身万里方看汗流血长安壮儿不敢骑走过掣电倾城知青丝络头为君老何由却出横门道吾闻天子之马走千里今之画图无乃是是何意态雄且杰骏尾萧梢朔风起毛为绿缥两耳黄眼有紫焰双瞳方矫矫龙性合变化卓立天骨森开张伊昔太仆张景顺监牧攻驹阅清峻遂令大奴守天育别养骥子怜神俊当时四十万匹马张公叹其材尽下故独写真传世人见之座右久更新年多物化空形影呜呼健步无由骋如今岂无騕褭与骅骝时无王良伯乐死即休缫丝须长不须白越罗蜀锦金粟尺象床玉手乱殷红万草千花动凝碧已悲素质随时染裂下鸣机色相射美人细意熨帖平裁缝灭尽针线迹春天衣著为君舞蛱蝶飞来黄鹂语落絮游丝亦有情随风照日宜轻举香汗轻尘污颜色开新合故置何许君不见才士汲引难恐惧弃捐忍羁旅雨中百草秋烂死阶下决明颜色鲜著叶满枝翠羽盖开花无数黄金钱凉风萧萧吹汝急恐汝后时难独立堂上书生空白头临风三嗅馨香泣阑风长雨秋纷纷四海八荒同一云去马来牛不复辨浊泾清渭何当分禾头生耳黍穗黑农夫田妇无消息城中斗米换衾裯相许宁论两相直长安布衣谁比数反锁衡门守环堵老夫不出长蓬蒿稚子无忧走风雨雨声飕飕催早寒胡雁翅湿高飞难秋来未曾见白日泥污后土何时干檐前甘菊移时晚青蕊重阳不堪摘明日萧条醉尽醒残花烂熳开何益篱边野外多众芳采撷细琐升中堂念兹空长大枝叶结根失所缠风霜诸公衮衮登台省广文先生官独冷甲第纷纷厌粱肉广文先生饭不足先生有道出羲皇先生有才过屈宋德尊一代常轗轲名垂万古知何用杜陵野客人更嗤被褐短窄鬓如丝日籴太仓五升米时赴郑老同襟期得钱即相觅沽酒不复疑忘形到尔汝痛饮真吾师清夜沈沈动春酌灯前细雨檐花落但觉高歌有鬼神焉知饿死填沟壑相如逸才亲涤器子云识字终投阁先生早赋归去来石田茅屋荒苍苔儒术于我何有哉孔丘盗跖俱尘埃不须闻此意惨怆生前相遇且衔杯陆机二十作文赋汝更小年能缀文总角草书又神速世上儿子徒纷纷骅骝作驹已汗血鸷鸟举翮连青云词源倒流三峡水笔阵独扫千人军只今年才十六七射策君门期第一旧穿杨叶真自知暂蹶霜蹄未为失偶然擢秀非难取会是排风有毛质汝身已见唾成珠汝伯何由发如漆春光澹沱秦东亭渚蒲牙白水荇青风吹客衣日杲杲树搅离思花冥冥酒尽沙头双玉瓶众宾皆醉我独醒乃知贫贱别更苦吞声踯躅涕泪零人生不相见动如参与商今夕复何夕共此灯烛光少壮能几时鬓发各已苍访旧半为鬼惊呼热中肠焉知二十载重上君子堂昔别君未婚儿女忽成行怡然敬父执问我来何方问答乃未已儿女罗酒浆夜雨翦春韭新炊间黄粱主称会面难一举累十觞十觞亦不醉感子故意长明日隔山岳世事两茫茫今秋乃淫雨仲月来寒风群木水光下万象云气中所思碍行潦九里信不通悄悄素浐路迢迢天汉东愿腾六尺马背若孤征鸿划见公子面超然欢笑同奋飞既胡越局促伤樊笼一饭四五起凭轩心力穷嘉蔬没混浊时菊碎榛丛鹰隼亦屈猛乌鸢何所蒙式瞻北邻居取适南巷翁挂席钓川涨焉知清兴终高标跨苍天烈风无时休自非旷士怀登兹翻百忧方知象教力足可追冥搜仰穿龙蛇窟始出枝撑幽七星在北户河汉声西流羲和鞭白日少昊行清秋秦山忽破碎泾渭不可求俯视但一气焉能辨皇州回首叫虞舜苍梧云正愁惜哉瑶池饮日晏昆仑丘黄鹄去不息哀鸣何所投君看随阳雁各有稻粱谋平明跨驴出未知适谁门权门多噂er且复寻诸孙诸孙贫无事宅舍如荒村堂前自生竹堂后自生萱萱草秋已死竹枝霜不蕃淘米少汲水汲多井水浑刈葵莫放手放手伤葵根阿翁懒惰久觉儿行步奔所来为宗族亦不为盘飧小人利口实薄俗难可论勿受外嫌猜同姓古所敦出门复入门两脚但如旧所向泥活活思君令人瘦沉吟坐西轩饮食错昏昼寸步曲江头难为一相就吁嗟呼苍生稼穑不可救安得诛云师畴能补天漏大明韬日月旷野号禽兽君子强逶迤小人困驰骤维南有崇山恐与川浸溜是节东篱菊纷披为谁秀岑生多新诗性亦嗜醇酎采采黄金花何由满衣袖巢父掉头不肯住东将入海随烟雾诗卷长留天地间钓竿欲拂珊瑚树深山大泽龙蛇远春寒野阴风景暮蓬莱织女回云车指点虚无是征路自是君身有仙骨世人那得知其故惜君只欲苦死留富贵何如草头露蔡侯静者意有馀清夜置酒临前除罢琴惆怅月照席几岁寄我空中书南寻禹穴见李白道甫问信今何如知章骑马似乘船眼花落井水底眠汝阳三斗始朝天道逢麹车口流涎恨不移封向酒泉左相日兴费万钱饮如长鲸吸百川衔杯乐圣称世贤宗之潇洒美少年举觞白眼望青天皎如玉树临风前苏晋长斋绣佛前醉中往往爱逃禅李白一斗诗百篇长安市上酒家眠天子呼来不上船自称臣是酒中仙张旭三杯草圣传脱帽露顶王公前挥毫落纸如云烟焦遂五斗方卓然高谈雄辨惊四筵曲江萧条秋气高菱荷枯折随风涛游子空嗟垂二毛白石素沙亦相荡哀鸿独叫求其曹即事非今亦非古长歌激越梢林莽比屋豪华固难数吾人甘作心似灰弟侄何伤泪如雨自断此生休问天杜曲幸有桑麻田故将移住南山边短衣匹马随李广看射猛虎终残年三月三日天气新长安水边多丽人态浓意远淑且真肌理细腻骨肉匀绣罗衣裳照暮春蹙金孔雀银麒麟头上何所有翠微zc叶垂鬓唇背后何所见珠压腰衱稳称身就中云幕椒房亲赐名大国虢与秦紫驼之峰出翠釜水精之盘行素鳞犀箸厌饫久未下銮刀缕切空纷纶黄门飞鞚不动尘御厨络绎送八珍箫鼓哀吟感鬼神宾从杂遝实要津后来鞍马何逡巡当轩下马入锦茵杨花雪落覆白蘋青鸟飞去衔红巾炙手可热势绝伦慎莫近前丞相嗔乐游古园崒森爽烟绵碧草萋萋长公子华筵势最高秦川对酒平如掌长生木瓢示真率更调鞍马狂欢赏青春波浪芙蓉园白日雷霆夹城仗阊阖晴开昳荡荡曲江翠幕排银榜拂水低徊舞袖翻缘云清切歌声上却忆年年人醉时只今未醉已先悲数茎白发那抛得百罚深杯亦不辞圣朝亦知贱士丑一物自荷皇天慈此身饮罢无归处独立苍茫自咏诗岑参兄弟皆好奇携我远来游渼陂天地黤惨忽异色波涛万顷堆琉璃琉璃汗漫泛舟入事殊兴极忧思集鼍作鲸吞不复知恶风白浪何嗟及主人锦帆相为开舟子喜甚无氛埃凫鹥散乱棹讴发丝管啁啾空翠来沈竿续蔓深莫测菱叶荷花静如拭宛在中流渤澥清下归无极终南黑半陂已南纯浸山动影袅窕冲融间船舷暝戛云际寺水面月出蓝田关此时骊龙亦吐珠冯夷击鼓群龙趋湘妃汉女出歌舞金支翠旗光有无咫尺但愁雷雨至苍茫不晓神灵意少壮几时奈老何向来哀乐何其多高台面苍陂六月风日冷蒹葭离披去天水相与永怀新目似击接要心已领仿像识鲛人空蒙辨鱼艇错磨终南翠颠倒白阁影崷崒增光辉乘陵惜俄顷劳生愧严郑外物慕张邴世复轻骅骝吾甘杂蛙黾知归俗可忽取适事莫并身退岂待官老来苦便静况资菱芡足庶结茅茨迥从此具扁舟弥年逐清景广文到官舍系马堂阶下醉则骑马归颇遭官长骂才名四十年坐客寒无毡赖有苏司业时时与酒钱远林暑气薄公子过我游贫居类村坞僻近城南楼旁舍颇淳朴所愿亦易求隔屋唤西家借问有酒不墙头过浊醪展席俯长流清风左右至客意已惊秋巢多众鸟斗叶密鸣蝉稠苦道此物聒孰谓吾庐幽水花晚色静庶足充淹留预恐尊中尽更起为君谋东山气鸿濛宫殿居上头君来必十月树羽临九州阴火煮玉泉喷薄涨岩幽有时浴赤日光抱空中楼阆风入辙迹旷原延冥搜沸天万乘动观水百丈湫幽灵斯可佳王命官属休初闻龙用壮擘石摧林丘中夜窟宅改移因风雨秋倒悬瑶池影屈注苍江流味如甘露浆挥弄滑且柔翠旗澹偃蹇云车纷少留箫鼓荡四溟异香泱漭浮鲛人献微绡曾祝沈豪牛百祥奔盛明古先莫能俦坡陀金虾蟆出见盖有由至尊顾之笑王母不肯收复归虚无底化作长黄虬飘飘青琐郎文彩珊瑚钩浩歌渌水曲清绝听者愁许生五台宾业白出石壁余亦师粲可身犹缚禅寂何阶子方便谬引为匹敌离索晚相逢包蒙欣有击诵诗浑游衍四座皆辟易应手看捶钩清心听鸣镝精微穿溟涬飞动摧霹雳陶谢不枝梧风骚共推激紫燕自超诣翠驳谁剪剔君意人莫知人间夜寥阒先帝昔晏驾兹山朝百灵崇冈拥象设沃野开天庭即事壮重险论功超五丁坡陀因厚地却略罗峻屏云阙虚冉冉风松肃泠泠石门霜露白玉殿莓苔青宫女晚知曙祠官朝见星空梁簇画戟阴井敲铜瓶中使日夜继惟王心不宁岂徒恤备享尚谓求无形孝理敦国政神凝推道经瑞芝产庙柱好鸟鸣岩扃高岳前嵂崒洪河左滢濙金城蓄峻址沙苑交回汀永与奥区固川原纷眇冥居然赤县立台榭争岧亭官属果称是声华真可听王刘美竹润裴李春兰馨郑氏才振古啖侯笔不停遣辞必中律利物常发硎绮绣相展转琳琅愈青荧侧闻鲁恭化秉德崔瑗铭太史候凫影王乔随鹤翎朝仪限霄汉容思回林坰轗轲辞下杜飘飖陵浊泾诸生旧短褐旅泛一浮萍荒岁儿女瘦暮途涕泗零主人念老马廨署容秋萤流寓理岂惬穷愁醉未醒何当摆俗累浩荡乘沧溟君不见左辅白沙如白水缭以周墙百馀里龙媒昔是渥洼生汗血今称献于此苑中騋牝三千匹丰草青青寒不死食之豪健西域无每岁攻驹冠边鄙王有虎臣司苑门入门天厩皆云屯骕骦一骨独当御春秋二时归至尊至尊内外马盈亿伏枥在坰空大存逸群绝足信殊杰倜傥权奇难具论累累塠阜藏奔突往往坡陀纵超越角壮翻同麋鹿游浮深簸荡鼋鼍窟泉出巨鱼长比人丹砂作尾黄金鳞岂知异物同精气虽未成龙亦有神邓公马癖人共知初得花骢大宛种夙昔传闻思一见牵来左右神皆竦雄姿逸态何崷崒顾影骄嘶自矜宠隅目青荧夹镜悬肉骏碨礌连钱动朝来久试华轩下未觉千金满高价赤汗微生白雪毛银鞍却覆香罗帕卿家旧赐公取之天厩真龙此其亚昼洗须腾泾渭深朝趋可刷幽并夜吾闻良骥老始成此马数年人更惊岂有四蹄疾于鸟不与八骏俱先鸣时俗造次那得致云雾晦冥方降精近闻下诏喧都邑肯使骐驎地上行君不见鞲上鹰一饱则飞掣焉能作堂上燕衔泥附炎热野人旷荡无靦颜岂可久在王侯间未试囊中餐玉法明朝且入蓝田山杜陵有布衣老大意转拙许身一何愚窃比稷与契居然成濩落白首甘契阔盖棺事则已此志常觊豁穷年忧黎元叹息肠内热取笑同学翁浩歌弥激烈非无江海志萧洒送日月生逢尧舜君不忍便永诀当今廊庙具构厦岂云缺葵藿倾太阳物性固莫夺顾惟蝼蚁辈但自求其穴胡为慕大鲸辄拟偃溟渤以兹悟生理独耻事干谒兀兀遂至今忍为尘埃没终愧巢与由未能易其节沈饮聊自适放歌颇愁绝岁暮百草零疾风高冈裂天衢阴峥嵘客子中夜发霜严衣带断指直不得结凌晨过骊山御榻在嵽嵲蚩尤塞寒空蹴蹋崖谷滑瑶池气郁律羽林相摩戛君臣留欢娱乐动殷樛嶱赐浴皆长缨与宴非短褐彤庭所分帛本自寒女出鞭挞其夫家聚敛贡城阙圣人筐篚恩实欲邦国活臣如忽至理君岂弃此物多士盈朝廷仁者宜战栗况闻内金盘尽在卫霍室中堂舞神仙烟雾散玉质暖客貂鼠裘悲管逐清瑟劝客驼蹄羹霜橙压香橘朱门酒肉臭路有冻死骨荣枯咫尺异惆怅难再述北辕就泾渭官渡又改辙群冰从西下极目高崒兀疑是崆峒来恐触天柱折河梁幸未坼枝撑声窸窣行旅相攀援川广不可越老妻寄异县十口隔风雪谁能久不顾庶往共饥渴入门闻号咷幼子饥已卒吾宁舍一哀里巷亦呜咽所愧为人父无食致夭折岂知秋未登贫窭有仓卒生常免租税名不隶征伐抚迹犹酸辛平人固骚屑默思失业徒因念远戍卒忧端齐终南澒洞不可掇堂上不合生枫树怪底江山起烟雾闻君扫却赤县图乘兴遣画沧洲趣画师亦无数好手不可遇对此融心神知君重毫素岂但祁岳与郑虔笔迹远过杨契丹得非悬圃裂无乃潇湘翻悄然坐我天姥下耳边已似闻清猿反思前夜风雨急乃是蒲城鬼神入元气淋漓障犹湿真宰上诉天应泣野亭春还杂花远渔翁暝蹋孤舟立沧浪水深青溟阔欹岸侧岛秋毫末不见湘妃鼓瑟时至今斑竹临江活刘侯天机精爱画入骨髓自有两儿郎挥洒亦莫比大儿聪明到能添老树巅崖里小儿心孔开貌得山僧及童子若耶溪云门寺吾独胡为在泥滓青鞋布袜从此始客从南县来浩荡无与适旅食白日长况当朱炎赫高斋坐林杪信宿游衍阒清晨陪跻攀傲睨俯峭壁崇冈相枕带旷野怀咫尺始知贤主人赠此遣愁寂危阶根青冥曾冰生淅沥上有无心云下有欲落石泉声闻复急动静随所击鸟呼藏其身有似惧弹射吏隐道性情兹焉其窟宅白水见舅氏诸翁乃仙伯杖藜长松阴作尉穷谷僻为我炊雕胡逍遥展良觌坐久风颇愁晚来山更碧相对十丈蛟欻翻盘涡坼何得空里雷殷殷寻地脉烟氛蔼崷崒魍魉森惨戚昆仑崆峒颠回首如不隔前轩颓反照巉绝华岳赤兵气涨林峦川光杂锋镝知是相公军铁马云雾积玉觞淡无味胡羯岂强敌长歌激屋梁泪下流衽席人生半哀乐天地有顺逆慨彼万国夫休明备征狄猛将纷填委庙谋蓄长策东郊何时开带甲且来释欲告清宴罢难拒幽明迫三叹酒食旁何由似平昔我经华原来不复见平陆北上唯土山连山走穷谷火云无时出飞电常在目自多穷岫雨行潦相豗蹙蓊匌川气黄群流会空曲清晨望高浪忽谓阴崖踣恐泥窜蛟龙登危聚麋鹿枯查卷拔树礧磈共充塞声吹鬼神下势阅人代速不有万穴归何以尊四渎及观泉源涨反惧江海覆漂沙坼岸去漱壑松柏秃乘陵破山门回斡裂地轴交洛赴洪河及关岂信宿应沈数州没如听万室哭秽浊殊未清风涛怒犹蓄何时通舟车阴气不黪黩浮生有荡汩吾道正羁束人寰难容身石壁滑侧足云雷此不已艰险路更跼普天无川梁欲济愿水缩因悲中林士未脱众鱼腹举头向苍天安得骑鸿鹄孟冬十郡良家子血作陈陶泽中水野旷天清无战声四万义军同日死群胡归来血洗箭仍唱胡歌饮都市都人回面向北啼日夜更望官军至我军青坂在东门天寒饮马太白窟黄头奚儿日向西数骑弯弓敢驰突山雪河冰野萧瑟青是烽烟白人骨焉得附书与我军忍待明年莫仓卒少陵野老吞声哭春日潜行曲江曲江头宫殿锁千门细柳新蒲为谁绿忆昔霓旌下南苑苑中万物生颜色昭阳殿里第一人同辇随君侍君侧辇前才人带弓箭白马嚼啮黄金勒翻身向天仰射云一箭正坠双飞翼明眸皓齿今何在血污游魂归不得清渭东流剑阁深去住彼此无消息人生有情泪沾臆江水江花岂终极黄昏胡骑尘满城欲往城南忘南北长安城头头白乌夜飞延秋门上呼又向人家啄大屋屋底达官走避胡金鞭断折九马死骨肉不待同驰驱腰下宝玦青珊瑚可怜王孙泣路隅问之不肯道姓名但道困苦乞为奴已经百日窜荆棘身上无有完肌肤高帝子孙尽隆准龙种自与常人殊豺狼在邑龙在野王孙善保千金躯不敢长语临交衢且为王孙立斯须昨夜东风吹血腥东来橐驼满旧都朔方健儿好身手昔何勇锐今何愚窃闻天子已传位圣德北服南单于花门ko面请雪耻慎勿出口他人狙哀哉王孙慎勿疏五陵佳气无时无心在水精域衣沾春雨时洞门尽徐步深院果幽期到扉开复闭撞钟斋及兹醍醐长发性饮食过扶衰把臂有多日开怀无愧辞黄鹂度结构紫鸽下罘罳愚意会所适花边行自迟汤休起我病微笑索题诗细软青丝履光明白氎巾深藏供老宿取用及吾身自顾转无趣交情何尚新道林才不世惠远德过人雨泻暮檐竹风吹青井芹天阴对图画最觉润龙鳞灯影照无睡心清闻妙香夜深殿突兀风动金锒铛天黑闭春院地清栖暗芳玉绳回断绝铁凤森翱翔梵放时出寺钟残仍殷床明朝在沃野苦见尘沙黄童儿汲井华惯捷瓶上手沾洒不濡地扫除似无帚明霞烂复阁霁雾搴高牖侧塞被径花飘飖委墀柳艰难世事迫隐遁佳期后晤语契深心那能总箝口奉辞还杖策暂别终回首泱泱泥污人听听国多狗既未免羁绊时来憩奔走近公如白雪执热烦何有\n"
     ]
    }
   ],
   "source": [
    "print text.encode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=np.array(sentences).reshape(-1,timesteps)\n",
    "next_chars=np.array(next_chars).reshape(-1,timesteps)\n",
    "chars=indices_char\n",
    "vocab=char_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4436, 40, 64)\n",
      "(LSTMStateTuple(c=<tf.Tensor 'MultiRNNCellZeroState/LSTMCellZeroState/zeros:0' shape=(4436, 64) dtype=float32>, h=<tf.Tensor 'MultiRNNCellZeroState/LSTMCellZeroState/zeros_1:0' shape=(4436, 64) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'MultiRNNCellZeroState/LSTMCellZeroState_1/zeros:0' shape=(4436, 64) dtype=float32>, h=<tf.Tensor 'MultiRNNCellZeroState/LSTMCellZeroState_1/zeros_1:0' shape=(4436, 64) dtype=float32>))\n",
      "(4436, 40, 2511)\n",
      "(4436, 40)\n",
      "Tensor(\"ones:0\", shape=(4436, 40), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "graph = tf.Graph()\n",
    "batch_size=len(sentences)\n",
    "timesteps=40\n",
    "with graph.as_default():\n",
    "    train_inputs = tf.placeholder(tf.int32, shape=[batch_size,timesteps])\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[batch_size,timesteps])\n",
    "    \n",
    "    embeddings = tf.get_variable('embeddings',shape=[dictionary_size,embedding_size],\n",
    "                                 initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "    embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "#     embed = tf.unstack(embed, timesteps, 1)\n",
    "    softmax_weights = tf.get_variable('softmax_weights',shape=[num_hidden,dictionary_size],\n",
    "                                      initializer=tf.contrib.layers.xavier_initializer())\n",
    "    softmax_biases = tf.get_variable('softmax_biases',shape=[dictionary_size],initializer=tf.contrib.layers.xavier_initializer())\n",
    "    cells=[]\n",
    "    for i in range(2):\n",
    "        cell = rnn.LSTMCell(num_hidden)\n",
    "#         cell = rnn.DropoutWrapper(cell, output_keep_prob=0.2)\n",
    "        cells.append(cell)\n",
    "    lstm_cell = rnn.MultiRNNCell(cells, state_is_tuple=True)\n",
    "    print(embed.shape)\n",
    "    initial_state = lstm_cell.zero_state(batch_size, tf.float32)\n",
    "    print(initial_state)\n",
    "    outputs, final_state = tf.nn.dynamic_rnn(lstm_cell, embed, initial_state=initial_state,dtype=tf.float32)\n",
    "    outputs=tf.reshape(outputs,shape=[-1,embedding_size])\n",
    "    logits = tf.nn.xw_plus_b(outputs, softmax_weights, softmax_biases)\n",
    "    logits = tf.reshape(logits, [-1, timesteps, dictionary_size])\n",
    "    predict = tf.nn.softmax(logits)\n",
    "    print(logits.shape)\n",
    "    print(train_labels.shape)\n",
    "    print(tf.ones([batch_size,timesteps]))\n",
    "    loss = tf.contrib.seq2seq.sequence_loss(\n",
    "        logits,\n",
    "        train_labels,\n",
    "        tf.ones([batch_size,timesteps]))\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Model saved in path: ./generation_model/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "times=5000\n",
    "with tf.Session(graph=graph) as session:\n",
    "    \n",
    "    init.run()\n",
    "    print('Initialized')\n",
    "    average_loss = 0\n",
    "    for step in range(1,times+1):\n",
    "        feed_dict = {train_inputs: sentences, train_labels: next_chars}\n",
    "        _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        if step%1000==0:\n",
    "            saver.save(session, './generation_model/model.ckpt',\n",
    "                               global_step=step)\n",
    "    \n",
    "    print(\"Model saved in path: %s\" % './generation_model/model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mW0918 08:33:13.653621 Reloader plugin_event_accumulator.py:286] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[0m\u001b[33mW0918 08:33:13.653932 Reloader plugin_event_accumulator.py:294] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "\u001b[0m\u001b[33mW0918 08:33:13.659997 Reloader plugin_event_accumulator.py:286] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[0m\u001b[33mW0918 08:33:13.660279 Reloader plugin_event_accumulator.py:294] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "\u001b[0m\u001b[33mW0918 08:33:13.666134 Reloader plugin_event_accumulator.py:286] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[0m\u001b[33mW0918 08:33:13.666363 Reloader plugin_event_accumulator.py:294] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "\u001b[0m\u001b[33mW0918 08:33:13.668618 Reloader plugin_event_accumulator.py:286] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[0m\u001b[33mW0918 08:33:13.668858 Reloader plugin_event_accumulator.py:294] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "\u001b[0m\u001b[33mW0918 08:33:13.674770 Reloader plugin_event_accumulator.py:286] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "\u001b[0m\u001b[33mW0918 08:33:13.675006 Reloader plugin_event_accumulator.py:294] Found more than one metagraph event per run. Overwriting the metagraph with the newest event.\n",
      "\u001b[0mTensorBoard 1.9.0 at http://0d9bee726594:6006 (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir=output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(LSTMStateTuple(c=<tf.Tensor 'MultiRNNCellZeroState/LSTMCellZeroState/zeros:0' shape=(4436, 64) dtype=float32>, h=<tf.Tensor 'MultiRNNCellZeroState/LSTMCellZeroState/zeros_1:0' shape=(4436, 64) dtype=float32>),\n",
       " LSTMStateTuple(c=<tf.Tensor 'MultiRNNCellZeroState/LSTMCellZeroState_1/zeros:0' shape=(4436, 64) dtype=float32>, h=<tf.Tensor 'MultiRNNCellZeroState/LSTMCellZeroState_1/zeros_1:0' shape=(4436, 64) dtype=float32>))"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 64)\n",
      "(1, 1, 2511)\n",
      "(1, 1)\n",
      "Tensor(\"ones:0\", shape=(1, 1), dtype=float32)\n",
      "INFO:tensorflow:Restoring parameters from ./generation_model/model.ckpt-4000\n",
      "爱好海不何处潜\n",
      "贝旅万布堂相丈\n",
      "果花埃登涧感泉\n",
      "的绿泉愿生到已\n",
      "大西虞人衢春草\n",
      "金敬竹朋珠溪枝\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "graph = tf.Graph()\n",
    "batch_size=1\n",
    "timesteps=1\n",
    "num=6\n",
    "\n",
    "sampling_type=0\n",
    "\n",
    "with graph.as_default():\n",
    "    train_inputs = tf.placeholder(tf.int32, shape=[batch_size,timesteps])\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[batch_size,timesteps])\n",
    "\n",
    "    embeddings = tf.get_variable('embeddings',shape=[dictionary_size,embedding_size],\n",
    "                                 initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "    embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "    #     embed = tf.unstack(embed, timesteps, 1)\n",
    "    softmax_weights = tf.get_variable('softmax_weights',shape=[num_hidden,dictionary_size],\n",
    "                                      initializer=tf.contrib.layers.xavier_initializer())\n",
    "    softmax_biases = tf.get_variable('softmax_biases',shape=[dictionary_size],initializer=tf.contrib.layers.xavier_initializer())\n",
    "    cells=[]\n",
    "    for i in range(2):\n",
    "        cell = rnn.LSTMCell(num_hidden)\n",
    "        cells.append(cell)\n",
    "    lstm_cell = rnn.MultiRNNCell(cells, state_is_tuple=True)\n",
    "    print(embed.shape)\n",
    "    initial_state = lstm_cell.zero_state(batch_size, tf.float32)\n",
    "    outputs, final_state = tf.nn.dynamic_rnn(lstm_cell, embed, initial_state=initial_state,dtype=tf.float32)\n",
    "    outputs=tf.reshape(outputs,shape=[-1,embedding_size])\n",
    "    logits = tf.nn.xw_plus_b(outputs, softmax_weights, softmax_biases)\n",
    "    logits = tf.reshape(logits, [-1, timesteps, dictionary_size])\n",
    "    predict = tf.nn.softmax(logits)\n",
    "    print(logits.shape)\n",
    "    print(train_labels.shape)\n",
    "    print(tf.ones([batch_size,timesteps]))\n",
    "    loss = tf.contrib.seq2seq.sequence_loss(\n",
    "        logits,\n",
    "        train_labels,\n",
    "        tf.ones([batch_size,timesteps]))\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as session:\n",
    "        ckpt = tf.train.get_checkpoint_state('./generation_model/')\n",
    "        def weighted_pick(weights):\n",
    "            t = np.cumsum(weights)\n",
    "            s = np.sum(weights)\n",
    "            return(int(np.searchsorted(t, np.random.rand(1)*s)))\n",
    "        \n",
    "#         for c in ckpt.all_model_checkpoint_paths:\n",
    "        for c in ['./generation_model/model.ckpt-4000']:\n",
    "            saver = tf.train.Saver(tf.global_variables())\n",
    "            saver.restore(session, c)\n",
    "            state = session.run(lstm_cell.zero_state(1, tf.float32))\n",
    "            for prime in u'爱贝果的大金':\n",
    "#             for prime in [u'空山新雨后']:\n",
    "                for p in prime[:-1]:\n",
    "                    x = np.zeros((1, 1))\n",
    "                    x[0, 0] = vocab[p]\n",
    "                    feed = {train_inputs: x,initial_state:state}\n",
    "                    [pre,state]=session.run([predict, final_state], feed)\n",
    "                ret = prime\n",
    "                char = prime[-1]\n",
    "\n",
    "                for _ in range(num):\n",
    "                    x = np.zeros((1, 1))\n",
    "                    x[0, 0] = vocab[char]\n",
    "                    feed = {train_inputs: x,initial_state:state}\n",
    "                    [probs, state] = session.run([predict, final_state], feed)\n",
    "                    p = probs[0][0]\n",
    "\n",
    "                    if sampling_type == 0:\n",
    "                        sample = np.argmax(p)\n",
    "                    elif sampling_type == 2:\n",
    "                        if char == ' ':\n",
    "                            sample = weighted_pick(p)\n",
    "                        else:\n",
    "                            sample = np.argmax(p)\n",
    "                    else:  # sampling_type == 1 default:\n",
    "\n",
    "                        sample = weighted_pick(p)\n",
    "                    pred = chars[sample]\n",
    "                    ret += pred\n",
    "                    char = pred\n",
    "                print(ret)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u'人溪鹤树岩' in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'./generation_model/model.ckpt-1', u'./generation_model/model.ckpt-10', u'./generation_model/model.ckpt-100', u'./generation_model/model.ckpt-1000', u'./generation_model/model.ckpt-10000']"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt.all_model_checkpoint_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4436, 40, 64)\n",
      "(LSTMStateTuple(c=<tf.Tensor 'MultiRNNCellZeroState/LSTMCellZeroState/zeros:0' shape=(4436, 64) dtype=float32>, h=<tf.Tensor 'MultiRNNCellZeroState/LSTMCellZeroState/zeros_1:0' shape=(4436, 64) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'MultiRNNCellZeroState/LSTMCellZeroState_1/zeros:0' shape=(4436, 64) dtype=float32>, h=<tf.Tensor 'MultiRNNCellZeroState/LSTMCellZeroState_1/zeros_1:0' shape=(4436, 64) dtype=float32>))\n",
      "40\n",
      "(4436, 64)\n",
      "40\n",
      "(4436, 64)\n",
      "--------\n",
      "40\n",
      "(4436, 2511)\n",
      "40\n",
      "(4436,)\n",
      "40\n",
      "(4436,)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "graph = tf.Graph()\n",
    "batch_size=len(sentences)\n",
    "timesteps=40\n",
    "training=True\n",
    "with graph.as_default():\n",
    "    train_inputs = tf.placeholder(tf.int32, shape=[batch_size,timesteps])\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[batch_size,timesteps])\n",
    "    \n",
    "    embeddings = tf.get_variable('embeddings',shape=[dictionary_size,embedding_size],\n",
    "                                 initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "    embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "#     embed = tf.unstack(embed, timesteps, 1)\n",
    "    softmax_weights = tf.get_variable('softmax_weights',shape=[num_hidden,dictionary_size],\n",
    "                                      initializer=tf.contrib.layers.xavier_initializer())\n",
    "    softmax_biases = tf.get_variable('softmax_biases',shape=[dictionary_size],initializer=tf.contrib.layers.xavier_initializer())\n",
    "    cells=[]\n",
    "    for _ in range(2):\n",
    "        cell = rnn.LSTMCell(embedding_size)\n",
    "#         cell = rnn.DropoutWrapper(cell, output_keep_prob=0.2)\n",
    "        cells.append(cell)\n",
    "    cell = rnn.MultiRNNCell(cells, state_is_tuple=True)\n",
    "    print(embed.shape)\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    print(initial_state)\n",
    "    inputs = tf.split(embed, timesteps, 1)\n",
    "    inputs = [tf.squeeze(input_, [1]) for input_ in inputs]\n",
    "    print(len(inputs))\n",
    "    print(inputs[0].shape)\n",
    "    def loop(prev, _):\n",
    "        prev = tf.matmul(prev, softmax_weights) + softmax_biases\n",
    "        prev_symbol = tf.stop_gradient(tf.argmax(prev, 1))\n",
    "        return tf.nn.embedding_lookup(embeddings, prev_symbol)\n",
    "\n",
    "    # rnn_decoder to generate the ouputs and final state. When we are not training the model, we use the loop function.\n",
    "    outputs, final_state = tf.contrib.legacy_seq2seq.rnn_decoder(inputs, initial_state, cell, loop_function=loop if not training else None, scope='rnnlm')\n",
    "    output = tf.reshape(tf.concat(outputs, 1), [-1, embedding_size])\n",
    "    \n",
    "    print(len(outputs))\n",
    "    print(outputs[0].shape)\n",
    "    logits = tf.nn.xw_plus_b(output, softmax_weights, softmax_biases)\n",
    "    logits = tf.reshape(logits, [-1, timesteps, dictionary_size])\n",
    "    predict = tf.nn.softmax(logits)\n",
    "    print('-'*8)\n",
    "    \n",
    "    \n",
    "    logits = tf.split(logits, timesteps, 1)\n",
    "    logits = [tf.squeeze(input_, [1]) for input_ in logits]\n",
    "    print(len(logits))\n",
    "    print(logits[0].get_shape())\n",
    "    labels = tf.split(train_labels, timesteps, 1)\n",
    "    labels = [tf.squeeze(input_, [1]) for input_ in labels]\n",
    "    print(len(labels))\n",
    "    print(labels[0].shape)\n",
    "    loss_weights = [tf.ones([batch_size]) for i in range(timesteps)]\n",
    "    print(len(loss_weights))\n",
    "    print(loss_weights[0].shape)\n",
    "    loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example(\n",
    "        logits,\n",
    "        labels,\n",
    "        loss_weights)\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Model saved in path: ./rnn_decoder/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "times=5000\n",
    "with tf.Session(graph=graph) as session:\n",
    "    \n",
    "    init.run()\n",
    "    print('Initialized')\n",
    "    average_loss = 0\n",
    "    for step in range(1,times+1):\n",
    "        feed_dict = {train_inputs: sentences, train_labels: next_chars}\n",
    "        _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        if step%1000==0:\n",
    "            saver.save(session, './rnn_decoder/model.ckpt',\n",
    "                               global_step=step)\n",
    "    print(\"Model saved in path: %s\" % './rnn_decoder/model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 64)\n",
      "1\n",
      "(1, 64)\n",
      "1\n",
      "(1, 64)\n",
      "--------\n",
      "1\n",
      "(1, 2511)\n",
      "1\n",
      "(1,)\n",
      "1\n",
      "(1,)\n",
      "INFO:tensorflow:Restoring parameters from ./rnn_decoder/model.ckpt-3000\n",
      "(LSTMStateTuple(c=array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "      dtype=float32), h=array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "      dtype=float32)), LSTMStateTuple(c=array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "      dtype=float32), h=array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "      dtype=float32)))\n",
      "爱好何处尊\n",
      "贝宋长能驯\n",
      "果宽惊匹弹\n",
      "的无风草寒\n",
      "大碎鼓仁忘\n",
      "金虑称会何\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "graph = tf.Graph()\n",
    "batch_size=1\n",
    "timesteps=1\n",
    "training=False\n",
    "num=4\n",
    "\n",
    "sampling_type=0\n",
    "with graph.as_default():\n",
    "    train_inputs = tf.placeholder(tf.int32, shape=[batch_size,timesteps])\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[batch_size,timesteps])\n",
    "\n",
    "    embeddings = tf.get_variable('embeddings',shape=[dictionary_size,embedding_size],\n",
    "                                 initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "    embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "#     embed = tf.unstack(embed, timesteps, 1)\n",
    "    softmax_weights = tf.get_variable('softmax_weights',shape=[num_hidden,dictionary_size],\n",
    "                                      initializer=tf.contrib.layers.xavier_initializer())\n",
    "    softmax_biases = tf.get_variable('softmax_biases',shape=[dictionary_size],initializer=tf.contrib.layers.xavier_initializer())\n",
    "    cells=[]\n",
    "    for _ in range(2):\n",
    "        cell = rnn.LSTMCell(embedding_size)\n",
    "        cells.append(cell)\n",
    "    cell = rnn.MultiRNNCell(cells, state_is_tuple=True)\n",
    "    print(embed.shape)\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "    inputs = tf.split(embed, timesteps, 1)\n",
    "    inputs = [tf.squeeze(input_, [1]) for input_ in inputs]\n",
    "    print(len(inputs))\n",
    "    print(inputs[0].shape)\n",
    "    def loop(prev, _):\n",
    "        prev = tf.matmul(prev, softmax_weights) + softmax_biases\n",
    "        prev_symbol = tf.stop_gradient(tf.argmax(prev, 1))\n",
    "        return tf.nn.embedding_lookup(embeddings, prev_symbol)\n",
    "\n",
    "    # rnn_decoder to generate the ouputs and final state. When we are not training the model, we use the loop function.\n",
    "    outputs, final_state = tf.contrib.legacy_seq2seq.rnn_decoder(inputs, initial_state, cell, loop_function=loop if not training else None, scope='rnnlm')\n",
    "    output = tf.reshape(tf.concat(outputs, 1), [-1, embedding_size])\n",
    "\n",
    "    print(len(outputs))\n",
    "    print(outputs[0].shape)\n",
    "    logits = tf.nn.xw_plus_b(output, softmax_weights, softmax_biases)\n",
    "    logits = tf.reshape(logits, [-1, timesteps, dictionary_size])\n",
    "    predict = tf.nn.softmax(logits)\n",
    "    print('-'*8)\n",
    "\n",
    "\n",
    "    logits = tf.split(logits, timesteps, 1)\n",
    "    logits = [tf.squeeze(input_, [1]) for input_ in logits]\n",
    "    print(len(logits))\n",
    "    print(logits[0].get_shape())\n",
    "    labels = tf.split(train_labels, timesteps, 1)\n",
    "    labels = [tf.squeeze(input_, [1]) for input_ in labels]\n",
    "    print(len(labels))\n",
    "    print(labels[0].shape)\n",
    "    loss_weights = [tf.ones([batch_size]) for i in range(timesteps)]\n",
    "    print(len(loss_weights))\n",
    "    print(loss_weights[0].shape)\n",
    "    loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example(\n",
    "        logits,\n",
    "        labels,\n",
    "        loss_weights)\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "    init = tf.global_variables_initializer()\n",
    "#     saver = tf.train.Saver()\n",
    "#     saver = tf.train.import_meta_graph('./generation_model_rnn_decoder.meta')\n",
    "    with tf.Session() as session:\n",
    "        ckpt = tf.train.get_checkpoint_state('./rnn_decoder/')\n",
    "\n",
    "        def weighted_pick(weights):\n",
    "            t = np.cumsum(weights)\n",
    "            s = np.sum(weights)\n",
    "            return(int(np.searchsorted(t, np.random.rand(1)*s)))\n",
    "\n",
    "#         for c in ckpt.all_model_checkpoint_paths:\n",
    "        for c in ['./rnn_decoder/model.ckpt-3000']:\n",
    "            saver = tf.train.Saver(tf.global_variables())\n",
    "            saver.restore(session, c)\n",
    "            state = session.run(cell.zero_state(1, tf.float32))\n",
    "            for prime in u'爱贝果的大金':\n",
    "                for char in prime[:-1]:\n",
    "                    x = np.zeros((1, 1))\n",
    "                    x[0, 0] = vocab[char]\n",
    "                    feed = {train_inputs: x, initial_state: state}\n",
    "                    [state] = session.run([final_state], feed)\n",
    "\n",
    "                ret = prime\n",
    "                char = prime[-1]\n",
    "                for _ in range(num):\n",
    "                    x = np.zeros((1, 1))\n",
    "                    x[0, 0] = vocab[char]\n",
    "                    feed = {train_inputs: x, initial_state: state}\n",
    "                    [probs, state] = session.run([predict, final_state], feed)\n",
    "                    p = probs[0][0]\n",
    "\n",
    "                    if sampling_type == 0:\n",
    "                        sample = np.argmax(p)\n",
    "                    elif sampling_type == 2:\n",
    "                        if char == u'。':\n",
    "                            sample = weighted_pick(p)\n",
    "                        else:\n",
    "                            sample = np.argmax(p)\n",
    "                    else:  # sampling_type == 1 default:\n",
    "\n",
    "                        sample = weighted_pick(p)\n",
    "                    pred = chars[sample]\n",
    "                    ret += pred\n",
    "                    char = pred\n",
    "                print(ret)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u'相遇' in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
